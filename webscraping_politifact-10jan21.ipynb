{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "API_ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsarika98/Major_Project_8th_sem/blob/main/webscraping_politifact-10jan21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MANy4T2gsoH3"
      },
      "source": [
        "#Webscraping using the library BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gsy9vWmt6HD"
      },
      "source": [
        "https://towardsdatascience.com/web-scraping-news-articles-in-python-9dd605799558"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S6_HKlAs1qT"
      },
      "source": [
        "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work\r\n",
        "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQCtmZkMsz6r"
      },
      "source": [
        "!pip install beautifulsoup4\r\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i8j-BuetftV"
      },
      "source": [
        "#importing neccessary packages\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "import time"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCk9y-yl9Dqm"
      },
      "source": [
        "#lists to store the scraped data\r\n",
        "authors=[]\r\n",
        "dates=[]\r\n",
        "statement=[]\r\n",
        "source=[]\r\n",
        "targets=[]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImX7Ur7Q4lP7"
      },
      "source": [
        "###Function to scrape the website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZU_56Sv9XgO"
      },
      "source": [
        "def scrape_website(pg_number):\r\n",
        "  pg_num = str(pg_number)\r\n",
        "  url = 'https://www.politifact.com/factchecks/list/?page='+pg_num  #politifact website\r\n",
        "  webpage = requests.get(url)\r\n",
        "  soup1 = BeautifulSoup(webpage.text, 'html.parser')\r\n",
        "\r\n",
        "  #Get teh location of the information\r\n",
        "  statement_footer = soup1.find_all('footer', attrs={'class':'m-statement__footer'})  #location of author\r\n",
        "  statement_quote = soup1.find_all('div', attrs={'class':'m-statement__quote'})       #location of statements\r\n",
        "  statement_meta = soup1.find_all('div', attrs={'class':'m-statement__meta'})         #location of source\r\n",
        "  target = soup1.find_all('div', attrs={'class':'m-statement__meter'})                #location of target(score card)\r\n",
        "\r\n",
        "  #loop for statement_footer\r\n",
        "  for i in statement_footer:\r\n",
        "     link1 = i.text.strip()\r\n",
        "     name_and_date = link1.split()\r\n",
        "     firstname = name_and_date[1]\r\n",
        "     lastname = name_and_date[2]\r\n",
        "     fullname = firstname+' '+lastname\r\n",
        "     month = name_and_date[4]\r\n",
        "     day = name_and_date[5]\r\n",
        "     year = name_and_date[6]\r\n",
        "     date = month+' '+day+' '+year\r\n",
        "     dates.append(date)\r\n",
        "     authors.append(fullname)\r\n",
        "\r\n",
        "  #loop for statement_quote\r\n",
        "  for i in statement_quote:\r\n",
        "     link2= i.find_all('a')\r\n",
        "     txt= link2[0].text.strip()\r\n",
        "     statement.append(txt)\r\n",
        "\r\n",
        "  #loop for statement_meta\r\n",
        "  for i in statement_meta:\r\n",
        "     link3= i.find_all('a')\r\n",
        "     src= link3[0].text.strip()\r\n",
        "     source.append(src)\r\n",
        "      \r\n",
        "  #loop for target\r\n",
        "  for i in target:\r\n",
        "     link4= i.find('div', attrs={'class':'c-image'}).find('img').get('alt')\r\n",
        "     targets.append(link4)\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m77oaU7l491G"
      },
      "source": [
        "Loop through 'n-1' webpages to scrape the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrd-SyLW428M"
      },
      "source": [
        "n = 2 #to scrape 1 page\r\n",
        "for i in range(1, n):\r\n",
        "  scrape_website(i)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r94Y0338JdV"
      },
      "source": [
        "Creating Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKu9Q2W78HiM"
      },
      "source": [
        "data = pd.DataFrame(columns=['Author', 'Statement', 'Source', 'Date', 'Target'])\r\n",
        "data['Author']= authors\r\n",
        "data['Statement']= statement\r\n",
        "data['Source']= source\r\n",
        "data['Date']= dates\r\n",
        "data['Target']= targets\r\n",
        "data    #displays data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}